# LongCodeZip Environment Variables Example
# Copy this file to .env and fill in your actual API keys
# WARNING: Never commit .env file to git!

# ========================================
# LLM Provider API Keys
# ========================================

# DeepSeek API (https://platform.deepseek.com)
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# OpenAI API (https://platform.openai.com)
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic Claude API (https://console.anthropic.com)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google Gemini API (https://makersuite.google.com/app/apikey)
GOOGLE_API_KEY=your_google_api_key_here

# Azure OpenAI
AZURE_OPENAI_API_KEY=your_azure_openai_api_key_here
AZURE_OPENAI_ENDPOINT=your_azure_endpoint_here

# Qwen / Alibaba Cloud (https://dashscope.aliyun.com)
QWEN_API_KEY=your_qwen_api_key_here

# ========================================
# Local LLM Settings (No API key needed)
# ========================================

# Ollama (local, free)
# OLLAMA_HOST=http://localhost:11434
# OLLAMA_MODEL=llama3.1:8b

# LM Studio (local, free)
# LM_STUDIO_HOST=http://localhost:1234
# LM_STUDIO_MODEL=local-model

# llama.cpp (local, free)
# LLAMA_CPP_HOST=http://localhost:8080
# LLAMA_CPP_MODEL=model-name

# ========================================
# Configuration Options
# ========================================

# Default provider to use (deepseek, openai, anthropic, etc.)
DEFAULT_PROVIDER=deepseek

# Default compression rate (0.0 - 1.0)
DEFAULT_COMPRESSION_RATE=0.3

# Enable caching (true/false)
ENABLE_CACHE=true

# Cache TTL in seconds (default: 604800 = 7 days)
CACHE_TTL=604800

# Enable parallel processing (true/false)
ENABLE_PARALLEL=true

# Number of parallel threads (0 = auto)
PARALLEL_THREADS=0

# ========================================
# Development Settings
# ========================================

# Rust log level (error, warn, info, debug, trace)
RUST_LOG=info

# Enable debug output
DEBUG=false
