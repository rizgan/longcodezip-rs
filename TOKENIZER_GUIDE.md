# Tokenizer Guide

## –û–±–∑–æ—Ä

–° –≤–µ—Ä—Å–∏–∏ 0.3.0 LongCodeZip-rs –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **—Ç–æ—á–Ω—ã–π tokenizer** –Ω–∞ –æ—Å–Ω–æ–≤–µ tiktoken –≤–º–µ—Å—Ç–æ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø–æ–¥—Å—á–µ—Ç–∞.

## –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞

### –î–æ (v0.1.0 - v0.2.0)
```rust
// –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç: chars / 4
let count = text.chars().count() as f64 / 4.0;
// ‚ùå –ù–µ—Ç–æ—á–Ω–æ: –º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –Ω–∞ 20-50%
```

### –ü–æ—Å–ª–µ (v0.3.0+)
```rust
// –¢–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç —á–µ—Ä–µ–∑ tiktoken
let tokenizer = Tokenizer::from_model_name("gpt-4");
let count = tokenizer.count_tokens(text)?;
// ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å 100%
```

## –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏

### OpenAI
- **GPT-4, GPT-3.5-turbo** ‚Üí `cl100k_base`
- **GPT-4o** ‚Üí `o200k_base`
- **Code models (Codex)** ‚Üí `p50k_base`
- **GPT-3 (davinci, curie)** ‚Üí `r50k_base`

### –î—Ä—É–≥–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã
- **DeepSeek** ‚Üí `cl100k_base` (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ)
- **Claude (Anthropic)** ‚Üí `cl100k_base` (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ)
- **Custom models** ‚Üí `cl100k_base` (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)

## –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä tokenizer

```rust
use longcodezip::Tokenizer;

// –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–µ—Ä–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π tokenizer –¥–ª—è –º–æ–¥–µ–ª–∏
let tokenizer = Tokenizer::from_model_name("gpt-4");

let text = "def hello():\n    print('world')";
let count = tokenizer.count_tokens(text)?;

println!("Tokens: {}", count);
```

### –Ø–≤–Ω—ã–π –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏

```rust
use longcodezip::{Tokenizer, TokenizerModel};

// –Ø–≤–Ω–æ —É–∫–∞–∑–∞—Ç—å –º–æ–¥–µ–ª—å tokenizer
let tokenizer = Tokenizer::new(TokenizerModel::Cl100kBase);

let count = tokenizer.count_tokens(text)?;
```

## –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ

```rust
let tokenizer = Tokenizer::from_model_name("gpt-4");

// –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ç–æ–∫–µ–Ω—ã
let tokens = tokenizer.encode("Hello, world!")?;
println!("Tokens: {:?}", tokens); // [9906, 11, 1917, 0]

// –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ
let text = tokenizer.decode(&tokens)?;
println!("Text: {}", text); // "Hello, world!"
```

### Truncation (–æ–±—Ä–µ–∑–∫–∞)

```rust
let tokenizer = Tokenizer::from_model_name("gpt-4");

let long_text = "Very long text that needs to be truncated...";

// –û–±—Ä–µ–∑–∞—Ç—å –¥–æ 50 —Ç–æ–∫–µ–Ω–æ–≤
let truncated = tokenizer.truncate(long_text, 50)?;

// –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
let count = tokenizer.count_tokens(&truncated)?;
assert!(count <= 50);
```

### Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞

```rust
let tokenizer = Tokenizer::from_model_name("gpt-4");

let texts = vec![
    "First function",
    "Second function",
    "Third function",
];

// –ü–æ–¥—Å—á–∏—Ç–∞—Ç—å —Ç–æ–∫–µ–Ω—ã –¥–ª—è –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å—Ä–∞–∑—É
let counts = tokenizer.count_tokens_batch(&texts)?;

for (text, count) in texts.iter().zip(counts.iter()) {
    println!("{}: {} tokens", text, count);
}
```

## –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å LongCodeZip

### –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

LongCodeZip –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π tokenizer:

```rust
use longcodezip::{LongCodeZip, CompressionConfig, ProviderConfig};

// –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Å –º–æ–¥–µ–ª—å—é
let provider = ProviderConfig::openai("your-key", "gpt-4");

// Tokenizer –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –¥–ª—è gpt-4
let config = CompressionConfig::default()
    .with_provider(provider);

let compressor = LongCodeZip::new(config)?;
```

### –†–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏

```rust
// OpenAI GPT-4
let provider = ProviderConfig::openai("key", "gpt-4");
// –ò—Å–ø–æ–ª—å–∑—É–µ—Ç: cl100k_base

// OpenAI GPT-4o
let provider = ProviderConfig::openai("key", "gpt-4o");
// –ò—Å–ø–æ–ª—å–∑—É–µ—Ç: o200k_base

// DeepSeek
let provider = ProviderConfig::deepseek("key");
// –ò—Å–ø–æ–ª—å–∑—É–µ—Ç: cl100k_base

// Claude
let provider = ProviderConfig::claude("key", "claude-3-opus");
// –ò—Å–ø–æ–ª—å–∑—É–µ—Ç: cl100k_base
```

## –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

–†–∞–∑–Ω—ã–µ tokenizer'—ã –¥–∞—é—Ç —Ä–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤:

```rust
let code = r#"
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
"#;

// GPT-4 (cl100k_base):  29 tokens
// GPT-4o (o200k_base):  29 tokens
// Codex (p50k_base):    40 tokens
// GPT-3 (r50k_base):    50 tokens
```

## –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

### –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ

Tokenizer'—ã —Å–æ–∑–¥–∞—é—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –∏ –∫–µ—à–∏—Ä—É—é—Ç—Å—è:

```rust
use longcodezip::Tokenizer;

// –ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤ - –∑–∞–≥—Ä—É–∂–∞–µ—Ç tokenizer
let tokenizer1 = Tokenizer::from_model_name("gpt-4");

// –ü–æ—Å–ª–µ–¥—É—é—â–∏–µ –≤—ã–∑–æ–≤—ã - –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∫–µ—à
let tokenizer2 = Tokenizer::from_model_name("gpt-4");
```

### –°–∫–æ—Ä–æ—Å—Ç—å

–¢–æ—á–Ω—ã–π tokenizer –±—ã—Å—Ç—Ä–µ–µ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–≥–æ:

```
–ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π: ~0.01ms –Ω–∞ —Ç–µ–∫—Å—Ç
–¢–æ—á–Ω—ã–π tiktoken:  ~0.005ms –Ω–∞ —Ç–µ–∫—Å—Ç
```

## Fallback —Ä–µ–∂–∏–º

–ï—Å–ª–∏ tiktoken –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º:

```rust
use longcodezip::ApproximateTokenizer;

let tokenizer = ApproximateTokenizer::new();
let count = tokenizer.count_tokens("Hello world");
// –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ: 11 chars / 4 = 3 tokens
```

## –ü—Ä–∏–º–µ—Ä—ã

### –ü—Ä–∏–º–µ—Ä 1: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

```bash
cargo run --example tokenizer_demo
```

–í—ã–≤–æ–¥:
```
üìä Token counts by model:
GPT-4 (cl100k_base)       29 tokens
GPT-4o (o200k_base)       29 tokens
Codex (p50k_base)         40 tokens
GPT-3 (r50k_base)         50 tokens
```

### –ü—Ä–∏–º–µ—Ä 2: –ö–æ–º–ø—Ä–µ—Å—Å–∏—è —Å —Ç–æ—á–Ω—ã–º –ø–æ–¥—Å—á–µ—Ç–æ–º

```bash
cargo run --example demo
```

–†–µ–∑—É–ª—å—Ç–∞—Ç:
```
Original tokens:    402 (—Ç–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç)
Compressed tokens:  173
Compression ratio:  43.03%
```

## Best Practices

### 1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –º–æ–¥–µ–ª—å

```rust
// ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ
let provider = ProviderConfig::openai("key", "gpt-4");

// ‚ùå –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ - –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–æ–¥–µ–ª–∏
let provider = ProviderConfig::openai("key", "gpt-4");
let tokenizer = Tokenizer::new(TokenizerModel::R50kBase); // –î—Ä—É–≥–∞—è –º–æ–¥–µ–ª—å!
```

### 2. –ö–µ—à–∏—Ä—É–π—Ç–µ tokenizer

```rust
// ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ - —Å–æ–∑–¥–∞—Ç—å –æ–¥–∏–Ω —Ä–∞–∑
let tokenizer = Tokenizer::from_model_name("gpt-4");
for text in texts {
    tokenizer.count_tokens(text)?;
}

// ‚ùå –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ - —Å–æ–∑–¥–∞–≤–∞—Ç—å –∫–∞–∂–¥—ã–π —Ä–∞–∑
for text in texts {
    let tokenizer = Tokenizer::from_model_name("gpt-4");
    tokenizer.count_tokens(text)?;
}
```

### 3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ batch –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤

```rust
// ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ
let counts = tokenizer.count_tokens_batch(&texts)?;

// ‚ö†Ô∏è –ú–µ–Ω–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ
let counts: Vec<_> = texts.iter()
    .map(|t| tokenizer.count_tokens(t))
    .collect::<Result<Vec<_>>>()?;
```

## Troubleshooting

### –ü—Ä–æ–±–ª–µ–º–∞: –†–∞–∑–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö

**–†–µ—à–µ–Ω–∏–µ:** –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π tokenizer –¥–ª—è –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏.

```rust
// –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞–∫–æ–π tokenizer –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
let tokenizer = Tokenizer::from_model_name("gpt-4");
println!("Model: {}", tokenizer.model().name());
```

### –ü—Ä–æ–±–ª–µ–º–∞: –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–∏

**–†–µ—à–µ–Ω–∏–µ:** –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ç–æ–∫–µ–Ω—ã –≤–∞–ª–∏–¥–Ω—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ tokenizer'–∞.

```rust
let tokens = tokenizer.encode(text)?;
let decoded = tokenizer.decode(&tokens)?; // –î–æ–ª–∂–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å
```

## –ú–∏–≥—Ä–∞—Ü–∏—è —Å v0.2.0

### –î–æ

```rust
// –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç
let chars = text.chars().count();
let tokens = (chars as f64 / 4.0).ceil() as usize;
```

### –ü–æ—Å–ª–µ

```rust
// –¢–æ—á–Ω—ã–π –ø–æ–¥—Å—á–µ—Ç
let tokenizer = Tokenizer::from_model_name("gpt-4");
let tokens = tokenizer.count_tokens(text)?;
```

### –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –º–∏–≥—Ä–∞—Ü–∏—è

LongCodeZip –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—ã–π tokenizer, –Ω–∏–∫–∞–∫–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –∫–æ–¥–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è!

```rust
// –≠—Ç–æ—Ç –∫–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –æ–±–µ–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏
let compressor = LongCodeZip::new(config)?;
let result = compressor.compress_code(code, query, "").await?;
```

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è

- –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è tiktoken: https://github.com/openai/tiktoken
- OpenAI tokenizer info: https://platform.openai.com/tokenizer
- –ú–æ–¥–µ–ª–∏ –∏ –∏—Ö tokenizer'—ã: https://platform.openai.com/docs/models
